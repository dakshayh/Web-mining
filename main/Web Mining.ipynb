{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another source of data for analysis comes from web pages that can be scraped. Scraping is the art of extracting data from websites. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Components of a Website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Websites are created using HTML (Hypertext Markup Language), along with CSS (Cascading Style Sheets) and JavaScript. Here's what these things do ([source](https://blog.hubspot.com/marketing/web-design-html-css-javascript)):\n",
    "* HTML provides the basic structure of sites, which is enhanced and modified by other technologies like CSS and JavaScript.\n",
    "* CSS is used to control presentation, formatting, and layout.\n",
    "* JavaScript is used to control the behavior of different elements.\n",
    "\n",
    "In web scraping, we are mostly concerned with HTML. HTML contains components known as elements or \"HTML tags,\" which control how content within the tags are displayed in the browser. Types of tags include image tags, paragraph tags, header tags, and div tags. These tags have have attributes (such as \"class\" and \"id\" information), which we can reference to extract the data inside. You can view the source code of a web page by right clicking on it in your browser and clicking \"Inspect\" (Chrome).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try It\n",
    "\n",
    "Go to the URL: https://www.bikemap.net/en/r/98460/ and inspect the source code on the page. Specifically inspect the description of the route under \"About this route.\" What is the class of the div that contains the route information?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTTP Requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to fetch the HTML document for rendering, web browsers issue HTTP requests. We can simulate this action using the `requests` module. For your reference, the link to the `requests` documentation is below. \n",
    "\n",
    "https://requests.kennethreitz.org/en/master/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_url = 'https://www.bikemap.net/en/r/98460/'\n",
    "# Issue a simple HTTP request to get the webpage text\n",
    "bike_page = requests.get(bike_url)\n",
    "# Response code is returned\n",
    "bike_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use .text to view the page\n",
    "# View the first 1000 characters of the HTML document\n",
    "bike_page.text[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`requests` can also handle authentication and cookies. See the documentation for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also issue HTTP requests via the `urllib.request` module. This is the simplest way to download images. \n",
    "\n",
    "https://docs.python.org/3/library/urllib.request.html#module-urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will create an images folder in your current \n",
    "# working directory using the os module\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an images folder in the current working directory\n",
    "my_wd = os.getcwd()\n",
    "print(\"My working directory:\")\n",
    "print(my_wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to images folder will be the current working directory + images\n",
    "img_dir = os.path.join(my_wd,'images')\n",
    "# If images folder does not exist, create it\n",
    "if not os.path.exists(img_dir):\n",
    "    os.makedirs(img_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of image we want to retrieve\n",
    "bike_img_url = 'https://media.bikemap.net/routes/98460/staticmaps/98460_1000x260.jpg'\n",
    "\n",
    "# Use string functions to get the name of the image\n",
    "bike_img_name = bike_img_url.split(\"/\")[-1]\n",
    "print(bike_img_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use urllib to fetch the image\n",
    "# Save the image to the images folder in the working directory\n",
    "# Note that os.path.join is concatenating the path to the\n",
    "# images directory AND the name of the image e.g. my_image.png\n",
    "urllib.request.urlretrieve(url=bike_img_url, filename=os.path.join(img_dir,bike_img_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The HTML output we obtained from our HTTP request was not easy to read. We can use the `BeautifulSoup` module to make the HTTP more readable and to search for specific content on the page. We will barely scratch the surface of what you can do with BeautifulSoup. The documentation is below for reference.\n",
    "\n",
    "https://www.crummy.com/software/BeautifulSoup/bs4/doc/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Often people forget that the package name for BeautifulSoup is bs4\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall that our output from requests was not easy to read\n",
    "bike_page.text[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First pass the text from the http request to the parser \n",
    "bike_page_soup = BeautifulSoup(bike_page.text, 'html.parser')\n",
    "bike_page_soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# soup.find can search for specific kinds of tags (name),\n",
    "# classes, and ids\n",
    "# In the case of multiple matches, the first result will be returned\n",
    "bike_page_soup.find(name=\"h1\", class_=\"title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .find_all returns all matches as a list\n",
    "bike_page_soup.find_all(name=\"div\", class_=\"flag-body\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the output above, it looks like we don't need the \n",
    "# flag-body container divs - let's return the item-value spans instead\n",
    "# .find_all returns all matches in a list\n",
    "bike_values_list = bike_page_soup.find_all(name=\"span\", class_=\"item-value\")\n",
    "print(bike_values_list)\n",
    "print(\"Number of results: \" + str(len(bike_values_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's extract the text from inside route-distance\n",
    "# route-distance is the first result\n",
    "route_distance_str = bike_values_list[0].text\n",
    "print(route_distance_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equivalently, we could have used .find for the id \"route-distance\"\n",
    "print(bike_page_soup.find(name=\"span\", id=\"route-distance\").text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use regular expressions to extract the number!\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use re.findall to match only the numbers\n",
    "# Recall: \"\\d\" is a digit - \"+\" specifies 1 or more times\n",
    "re.findall('\\d+', route_distance_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the first result of the regular expression find as a float\n",
    "route_distance_num = float(re.findall('\\d.', route_distance_str)[0])\n",
    "print(route_distance_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice Problem\n",
    "\n",
    "At the beginning of class we inspected the element that contains the route description underneathe \"About this route.\" Recall what the div class was (or go look at it again). Extract the text from that particular div using BeautifulSoup.\n",
    "https://www.bikemap.net/en/r/98460 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Web Scraped Content in Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import nan as NA\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('max_colwidth',150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting It All Together - Define a Function to Process a Page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've inspected and extracted all the relevant information you want on the page, you can create a function to process the information from that URL. At a minimum, the function should take in the URL or a part of the URL as an argument. (In our example, we can use the route ID because that is a unique identifier of the URL.)\n",
    "\n",
    "Store the information in a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In our example, I will extract the title, description, distance, and image. Note I am using the route ID to construct the URL.\n",
    "def bike_url_scrape(route_id):\n",
    "    # Concatenate base URL with route ID to get specific page\n",
    "    bike_url = 'https://www.bikemap.net/en/r/' + str(route_id) + '/'\n",
    "\n",
    "    # Request the page HTML, pass to BeautifulSoup parser\n",
    "    bike_page = requests.get(bike_url)\n",
    "    bike_page_soup = BeautifulSoup(bike_page.text, 'html.parser')\n",
    "\n",
    "    # Title\n",
    "    bike_route_title_raw = bike_page_soup.find(name=\"h1\", class_=\"title\").text\n",
    "    # Strip leading/trailing whitespace from title\n",
    "    bike_route_title = bike_route_title_raw.strip()\n",
    "\n",
    "    # Extract the image - some pages do not have images!\n",
    "    # First we need to get the image url - it is inside the header carousel\n",
    "    try:    \n",
    "        carousel_div = bike_page_soup.find(name=\"div\", id=\"header-carousel\")\n",
    "        bike_img_url = carousel_div.find('img')['src']\n",
    "        bike_img_name = str(route_id) + \"_\" + bike_img_url.split(\"/\")[-1]\n",
    "        urllib.request.urlretrieve(url=bike_img_url, filename=os.path.join(img_dir,bike_img_name))\n",
    "    except:\n",
    "        print(\"Page does not have a header image.\")\n",
    "    \n",
    "    # Description\n",
    "    bike_route_desc = bike_page_soup.find(name=\"div\", class_=\"route-description\").text\n",
    "\n",
    "    # Distance string\n",
    "    bike_route_dist_str = bike_page_soup.find(name=\"span\", id=\"route-distance\").text\n",
    "\n",
    "    # Distance numeric - use regex\n",
    "    bike_route_dist_num = float(re.findall('\\d.', bike_route_dist_str)[0])\n",
    "    \n",
    "    # Return result as a dataframe created from a NP array\n",
    "    temp_row = np.array([[\n",
    "        route_id, bike_route_title, bike_route_desc, bike_route_dist_str, bike_route_dist_num\n",
    "        ]])\n",
    "    temp_df = pd.DataFrame(temp_row, columns = ['route_id', 'bike_route_title', 'bike_route_desc', 'bike_route_dist_str', 'bike_route_dist_num'])\n",
    "    \n",
    "    return temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try out some routes: 4994814, 4891998, 3503377, 1903803\n",
    "bike_scrape_df1 = bike_url_scrape(4994814)\n",
    "bike_scrape_df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop Over Pages and DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the time module to pause the code in case we are identified as scrapers and blocked!\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of route ids we want to scrape\n",
    "route_ids = [98460, 4994814, 4891998, 3503377, 1903803]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over our route IDs and append to the empty dataframe\n",
    "# Store dataframes in a list\n",
    "dfs = []\n",
    "for route_id in route_ids:\n",
    "    print(\"Processing Route: \" + str(route_id))\n",
    "    # Scrape route\n",
    "    bike_scrape_res = bike_url_scrape(route_id)\n",
    "    # Wait half a second\n",
    "    time.sleep(0.5)\n",
    "    # Append df to list\n",
    "    dfs.append(bike_scrape_res)\n",
    "    \n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pd.concat to combine the dataframes\n",
    "bike_scrape_df = pd.concat(dfs)\n",
    "bike_scrape_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Futher Analysis in Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A large set of methods we have already covered can then be applied to the data to answer questions.\n",
    "\n",
    "### Practice Problems\n",
    "How many kilometers of routes are available?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many km across all routes are available?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
