{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another source of data for analysis comes from web pages that can be scraped. Scraping is the art of extracting data from websites. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Components of a Website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Websites are created using HTML (Hypertext Markup Language), along with CSS (Cascading Style Sheets) and JavaScript. Here's what these things do ([source](https://blog.hubspot.com/marketing/web-design-html-css-javascript)):\n",
    "* HTML provides the basic structure of sites, which is enhanced and modified by other technologies like CSS and JavaScript.\n",
    "* CSS is used to control presentation, formatting, and layout.\n",
    "* JavaScript is used to control the behavior of different elements.\n",
    "\n",
    "In web scraping, we are mostly concerned with HTML. HTML contains components known as elements or \"HTML tags,\" which control how content within the tags are displayed in the browser. Types of tags include image tags, paragraph tags, header tags, and div tags. These tags have have attributes (such as \"class\" and \"id\" information), which we can reference to extract the data inside. You can view the source code of a web page by right clicking on it in your browser and clicking \"Inspect\" (Chrome).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try It\n",
    "\n",
    "Go to the URL: https://www.bikemap.net/en/r/98460/ and inspect the source code on the page. Specifically inspect the description of the route under \"About this route.\" What is the class of the div that contains the route information?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTTP Requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to fetch the HTML document for rendering, web browsers issue HTTP requests. We can simulate this action using the `requests` module. For your reference, the link to the `requests` documentation is below. \n",
    "\n",
    "https://requests.kennethreitz.org/en/master/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bike_url = 'https://www.bikemap.net/en/r/98460/'\n",
    "# Issue a simple HTTP request to get the webpage text\n",
    "bike_page = requests.get(bike_url)\n",
    "# Response code is returned\n",
    "bike_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\n\\n<!doctype html>\\n<html lang=\"en\">\\n    <head>\\n        <meta charset=\"utf-8\" />\\n        <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\" /><script type=\"text/javascript\">(window.NREUM||(NREUM={})).loader_config={licenseKey:\"a29ddca0f7\",applicationID:\"48546301\"};window.NREUM||(NREUM={}),__nr_require=function(e,n,t){function r(t){if(!n[t]){var i=n[t]={exports:{}};e[t][0].call(i.exports,function(n){var i=e[t][1][n];return r(i||n)},i,i.exports)}return n[t].exports}if(\"function\"==typeof __nr_require)return __nr_require;for(var i=0;i<t.length;i++)r(t[i]);return r}({1:[function(e,n,t){function r(){}function i(e,n,t){return function(){return o(e,[u.now()].concat(f(arguments)),n?null:this,t),n?void 0:this}}var o=e(\"handle\"),a=e(4),f=e(5),c=e(\"ee\").get(\"tracer\"),u=e(\"loader\"),s=NREUM;\"undefined\"==typeof window.newrelic&&(newrelic=s);var p=[\"setPageViewName\",\"setCustomAttribute\",\"setErrorHandler\",\"finished\",\"addToTrace\",\"inlineHit\",\"addRelease\"],l=\"api-\",d=l+\"ixn-\";a(p,function(e,n){s[n]=i('"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use .text to view the page\n",
    "# View the first 1000 characters of the HTML document\n",
    "bike_page.text[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`requests` can also handle authentication and cookies. See the documentation for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also issue HTTP requests via the `urllib.request` module. This is the simplest way to download images. \n",
    "\n",
    "https://docs.python.org/3/library/urllib.request.html#module-urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will create an images folder in your current \n",
    "# working directory using the os module\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My working directory:\n",
      "/Users/akshayd/Desktop/Sem_2/Python/Web Mining\n"
     ]
    }
   ],
   "source": [
    "# Create an images folder in the current working directory\n",
    "my_wd = os.getcwd()\n",
    "print(\"My working directory:\")\n",
    "print(my_wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to images folder will be the current working directory + images\n",
    "img_dir = os.path.join(my_wd,'images')\n",
    "# If images folder does not exist, create it\n",
    "if not os.path.exists(img_dir):\n",
    "    os.makedirs(img_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98460_1000x260.jpg\n"
     ]
    }
   ],
   "source": [
    "# URL of image we want to retrieve\n",
    "bike_img_url = 'https://media.bikemap.net/routes/98460/staticmaps/98460_1000x260.jpg'\n",
    "\n",
    "# Use string functions to get the name of the image\n",
    "bike_img_name = bike_img_url.split(\"/\")[-1]\n",
    "print(bike_img_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/Users/akshayd/Desktop/Sem_2/Python/Web Mining/images/98460_1000x260.jpg',\n",
       " <http.client.HTTPMessage at 0x106efb048>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use urllib to fetch the image\n",
    "# Save the image to the images folder in the working directory\n",
    "# Note that os.path.join is concatenating the path to the\n",
    "# images directory AND the name of the image e.g. my_image.png\n",
    "urllib.request.urlretrieve(url=bike_img_url, filename=os.path.join(img_dir,bike_img_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The HTML output we obtained from our HTTP request was not easy to read. We can use the `BeautifulSoup` module to make the HTTP more readable and to search for specific content on the page. We will barely scratch the surface of what you can do with BeautifulSoup. The documentation is below for reference.\n",
    "\n",
    "https://www.crummy.com/software/BeautifulSoup/bs4/doc/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Often people forget that the package name for BeautifulSoup is bs4\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\n\\n<!doctype html>\\n<html lang=\"en\">\\n    <head>\\n        <meta charset=\"utf-8\" />\\n        <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\" /><script type=\"text/javascript\">(window.NREUM||(NREUM={})).loader_config={licenseKey:\"a29ddca0f7\",applicationID:\"48546301\"};window.NREUM||(NREUM={}),__nr_require=function(e,n,t){function r(t){if(!n[t]){var i=n[t]={exports:{}};e[t][0].call(i.exports,function(n){var i=e[t][1][n];return r(i||n)},i,i.exports)}return n[t].exports}if(\"function\"==typeof __nr_require)return __nr_require;for(var i=0;i<t.length;i++)r(t[i]);return r}({1:[function(e,n,t){function r(){}function i(e,n,t){return function(){return o(e,[u.now()].concat(f(arguments)),n?null:this,t),n?void 0:this}}var o=e(\"handle\"),a=e(4),f=e(5),c=e(\"ee\").get(\"tracer\"),u=e(\"loader\"),s=NREUM;\"undefined\"==typeof window.newrelic&&(newrelic=s);var p=[\"setPageViewName\",\"setCustomAttribute\",\"setErrorHandler\",\"finished\",\"addToTrace\",\"inlineHit\",\"addRelease\"],l=\"api-\",d=l+\"ixn-\";a(p,function(e,n){s[n]=i('"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recall that our output from requests was not easy to read\n",
    "bike_page.text[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First pass the text from the http request to the parser \n",
    "bike_page_soup = BeautifulSoup(bike_page.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<h1 class=\"title\" itemprop=\"name\" title=\"Anacostia Trail \">\n",
       "         Anacostia Trail \n",
       "    </h1>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# soup.find can search for specific kinds of tags (name),\n",
    "# classes, and ids\n",
    "# In the case of multiple matches, the first result will be returned\n",
    "bike_page_soup.find(name=\"h1\", class_=\"title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<div class=\"flag-body\">\n",
       " <span class=\"item-value\" id=\"route-distance\">27 km</span>\n",
       " <span class=\"item-label\">Distance</span>\n",
       " </div>, <div class=\"flag-body\">\n",
       " <span class=\"item-value\" id=\"route-ascent\">\n",
       "                             126 m\n",
       "                     </span>\n",
       " <span class=\"item-label\">Ascent</span>\n",
       " </div>, <div class=\"flag-body\">\n",
       " <span class=\"item-value\" id=\"route-descent\">\n",
       "                             123 m\n",
       "                     </span>\n",
       " <span class=\"item-label\">Descent</span>\n",
       " </div>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# .find_all returns all matches as a list\n",
    "bike_page_soup.find_all(name=\"div\", class_=\"flag-body\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<span class=\"item-value\" id=\"route-distance\">27 km</span>, <span class=\"item-value\" id=\"route-ascent\">\n",
      "                            126 m\n",
      "                    </span>, <span class=\"item-value\" id=\"route-descent\">\n",
      "                            123 m\n",
      "                    </span>]\n",
      "Number of results: 3\n"
     ]
    }
   ],
   "source": [
    "# Looking at the output above, it looks like we don't need the \n",
    "# flag-body container divs - let's return the item-value spans instead\n",
    "# .find_all returns all matches in a list\n",
    "bike_values_list = bike_page_soup.find_all(name=\"span\", class_=\"item-value\")\n",
    "print(bike_values_list)\n",
    "print(\"Number of results: \" + str(len(bike_values_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 km\n"
     ]
    }
   ],
   "source": [
    "# Let's extract the text from inside route-distance\n",
    "# route-distance is the first result\n",
    "route_distance_str = bike_values_list[0].text\n",
    "print(route_distance_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 km\n"
     ]
    }
   ],
   "source": [
    "# Equivalently, we could have used .find for the id \"route-distance\"\n",
    "print(bike_page_soup.find(name=\"span\", id=\"route-distance\").text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use regular expressions to extract the number!\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['27']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use re.findall to match only the numbers\n",
    "# Recall: \"\\d\" is a digit - \"+\" specifies 1 or more times\n",
    "re.findall('\\d+', route_distance_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.0\n"
     ]
    }
   ],
   "source": [
    "# Store the first result of the regular expression find as a float\n",
    "route_distance_num = float(re.findall('\\d.', route_distance_str)[0])\n",
    "print(route_distance_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Web Scraped Content in Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import nan as NA\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('max_colwidth',150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting It All Together - Define a Function to Process a Page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've inspected and extracted all the relevant information you want on the page, you can create a function to process the information from that URL. At a minimum, the function should take in the URL or a part of the URL as an argument. (In our example, we can use the route ID because that is a unique identifier of the URL.)\n",
    "\n",
    "Store the information in a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In our example, I will extract the title, description, distance, and image. Note I am using the route ID to construct the URL.\n",
    "def bike_url_scrape(route_id):\n",
    "    # Concatenate base URL with route ID to get specific page\n",
    "    bike_url = 'https://www.bikemap.net/en/r/' + str(route_id) + '/'\n",
    "\n",
    "    # Request the page HTML, pass to BeautifulSoup parser\n",
    "    bike_page = requests.get(bike_url)\n",
    "    bike_page_soup = BeautifulSoup(bike_page.text, 'html.parser')\n",
    "\n",
    "    # Title\n",
    "    bike_route_title_raw = bike_page_soup.find(name=\"h1\", class_=\"title\").text\n",
    "    # Strip leading/trailing whitespace from title\n",
    "    bike_route_title = bike_route_title_raw.strip()\n",
    "\n",
    "    # Extract the image - some pages do not have images!\n",
    "    # First we need to get the image url - it is inside the header carousel\n",
    "    try:    \n",
    "        carousel_div = bike_page_soup.find(name=\"div\", id=\"header-carousel\")\n",
    "        bike_img_url = carousel_div.find('img')['src']\n",
    "        bike_img_name = str(route_id) + \"_\" + bike_img_url.split(\"/\")[-1]\n",
    "        urllib.request.urlretrieve(url=bike_img_url, filename=os.path.join(img_dir,bike_img_name))\n",
    "    except:\n",
    "        print(\"Page does not have a header image.\")\n",
    "    \n",
    "    # Description\n",
    "    bike_route_desc = bike_page_soup.find(name=\"div\", class_=\"route-description\").text\n",
    "\n",
    "    # Distance string\n",
    "    bike_route_dist_str = bike_page_soup.find(name=\"span\", id=\"route-distance\").text\n",
    "\n",
    "    # Distance numeric - use regex\n",
    "    bike_route_dist_num = float(re.findall('\\d.', bike_route_dist_str)[0])\n",
    "    \n",
    "    # Return result as a dataframe created from a NP array\n",
    "    temp_row = np.array([[\n",
    "        route_id, bike_route_title, bike_route_desc, bike_route_dist_str, bike_route_dist_num\n",
    "        ]])\n",
    "    temp_df = pd.DataFrame(temp_row, columns = ['route_id', 'bike_route_title', 'bike_route_desc', 'bike_route_dist_str', 'bike_route_dist_num'])\n",
    "    \n",
    "    return temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>route_id</th>\n",
       "      <th>bike_route_title</th>\n",
       "      <th>bike_route_desc</th>\n",
       "      <th>bike_route_dist_str</th>\n",
       "      <th>bike_route_dist_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4994814</td>\n",
       "      <td>West Hyattsville Metro to Roaming Rooster</td>\n",
       "      <td>\\n\\nNo description yet.\\n\\n</td>\n",
       "      <td>4 km</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  route_id                           bike_route_title  \\\n",
       "0  4994814  West Hyattsville Metro to Roaming Rooster   \n",
       "\n",
       "               bike_route_desc bike_route_dist_str bike_route_dist_num  \n",
       "0  \\n\\nNo description yet.\\n\\n                4 km                 4.0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try out some routes: 4994814, 4891998, 3503377, 1903803\n",
    "bike_scrape_df1 = bike_url_scrape(4994814)\n",
    "bike_scrape_df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop Over Pages and DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the time module to pause the code in case we are identified as scrapers and blocked!\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of route ids we want to scrape\n",
    "route_ids = [98460, 4994814, 4891998, 3503377, 1903803]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Route: 98460\n",
      "Processing Route: 4994814\n",
      "Processing Route: 4891998\n",
      "Processing Route: 3503377\n",
      "Page does not have a header image.\n",
      "Processing Route: 1903803\n",
      "Page does not have a header image.\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "# Loop over our route IDs and append to the empty dataframe\n",
    "# Store dataframes in a list\n",
    "dfs = []\n",
    "for route_id in route_ids:\n",
    "    print(\"Processing Route: \" + str(route_id))\n",
    "    # Scrape route\n",
    "    bike_scrape_res = bike_url_scrape(route_id)\n",
    "    # Wait half a second\n",
    "    time.sleep(0.5)\n",
    "    # Append df to list\n",
    "    dfs.append(bike_scrape_res)\n",
    "    \n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>route_id</th>\n",
       "      <th>bike_route_title</th>\n",
       "      <th>bike_route_desc</th>\n",
       "      <th>bike_route_dist_str</th>\n",
       "      <th>bike_route_dist_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>98460</td>\n",
       "      <td>Anacostia Trail</td>\n",
       "      <td>\\n\\nRoute for 2008 Anacostia Trails Fall Foliage Bike Ride on October 25. Mostly on paved trails. Detour through Macgruder Park because of WSSC wo...</td>\n",
       "      <td>27 km</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4994814</td>\n",
       "      <td>West Hyattsville Metro to Roaming Rooster</td>\n",
       "      <td>\\n\\nNo description yet.\\n\\n</td>\n",
       "      <td>4 km</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4891998</td>\n",
       "      <td>College Park Bike Run</td>\n",
       "      <td>\\n\\nFirst ride on my new bike.\\n\\n</td>\n",
       "      <td>22 km</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3503377</td>\n",
       "      <td>To Work</td>\n",
       "      <td>\\n\\nNo description yet.\\n\\n</td>\n",
       "      <td>9 km</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1903803</td>\n",
       "      <td>11/11/2012 Maryland trek</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\nDistance:\\n18.41 mi\\n\\n\\n\\n\\n</td>\n",
       "      <td>29 km</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  route_id                           bike_route_title  \\\n",
       "0    98460                            Anacostia Trail   \n",
       "0  4994814  West Hyattsville Metro to Roaming Rooster   \n",
       "0  4891998                      College Park Bike Run   \n",
       "0  3503377                                    To Work   \n",
       "0  1903803                   11/11/2012 Maryland trek   \n",
       "\n",
       "                                                                                                                                         bike_route_desc  \\\n",
       "0  \\n\\nRoute for 2008 Anacostia Trails Fall Foliage Bike Ride on October 25. Mostly on paved trails. Detour through Macgruder Park because of WSSC wo...   \n",
       "0                                                                                                                            \\n\\nNo description yet.\\n\\n   \n",
       "0                                                                                                                     \\n\\nFirst ride on my new bike.\\n\\n   \n",
       "0                                                                                                                            \\n\\nNo description yet.\\n\\n   \n",
       "0                                                                                                            \\n\\n\\n\\n\\n\\n\\nDistance:\\n18.41 mi\\n\\n\\n\\n\\n   \n",
       "\n",
       "  bike_route_dist_str bike_route_dist_num  \n",
       "0               27 km                27.0  \n",
       "0                4 km                 4.0  \n",
       "0               22 km                22.0  \n",
       "0                9 km                 9.0  \n",
       "0               29 km                29.0  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use pd.concat to combine the dataframes\n",
    "bike_scrape_df = pd.concat(dfs)\n",
    "bike_scrape_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
